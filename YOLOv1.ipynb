{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71cbb65e",
   "metadata": {},
   "source": [
    "# YOLOv1 implementation in PyTorch\n",
    "\n",
    "## Oshri Fatkiev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb7b933",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from torchvision.ops import nms, box_iou, distance_box_iou\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ea8b73",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d14369a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "EPOCHS = 100\n",
    "WARMUP_EPOCHS = 0\n",
    "\n",
    "LEARNING_RATE = 1e-4\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 5e-4\n",
    "\n",
    "EPSILON = 1e-6\n",
    "IMAGE_SIZE = (448,448) \n",
    "\n",
    "FLIPUD = 0.5\n",
    "FLIPLR = 0.5\n",
    "\n",
    "S = 7                        \n",
    "B = 2                        \n",
    "C = 1                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d550fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Albumentations:\n",
    "    def __init__(self):\n",
    "        self.transform = None\n",
    "        T = [\n",
    "            A.RandomRotate90(p=1),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.5),\n",
    "        ]\n",
    "        self.transform = A.Compose(T, bbox_params=A.BboxParams(format='yolo', label_fields=['class_labels']))\n",
    "\n",
    "    def __call__(self, img, labels, p=1.0):\n",
    "        if self.transform and np.random.random() < p:\n",
    "            new = self.transform(image=img, bboxes=labels[:, 1:], class_labels=labels[:, 0]) \n",
    "            img, labels = new['image'], np.array([[c, *b] for c, b in zip(new['class_labels'], new['bboxes'])])\n",
    "        return img, labels\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8593d861",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f8e31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YoloDataset(Dataset):\n",
    "    def __init__(self, images_path, labels_path, augment=False):\n",
    "        labels = sorted([f for f in glob.glob(f\"{labels_path}/*.txt\")])\n",
    "        \n",
    "        self.images, self.labels = [], []\n",
    "        for label in labels:\n",
    "            im = label.replace('labels', 'images').replace('txt', 'npy')\n",
    "            if os.path.exists(im):\n",
    "                self.labels.append(label)\n",
    "                self.images.append(im)\n",
    "\n",
    "        self.augment = augment\n",
    "        # self.albumentations = Albumentations() if self.augment else None\n",
    "        self.classes = {0 : 0}\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        # print(self.images[i])\n",
    "        img = np.load(self.images[i])\n",
    "        if img.shape != IMAGE_SIZE:\n",
    "            img = cv2.resize(img, dsize=IMAGE_SIZE, interpolation=cv2.INTER_CUBIC) # IMAGE_SIZE\n",
    "         \n",
    "        labels = np.loadtxt(self.images[i].replace('images', 'labels').replace('.npy', '.txt'))\n",
    "        labels = torch.tensor(labels)\n",
    "        if labels.dim() == 1:\n",
    "            labels = labels.unsqueeze(0)\n",
    "        \n",
    "        original_img = img\n",
    "        if self.augment:\n",
    "            # data, labels = self.albumentations(data, labels)\n",
    "            nl = len(labels) \n",
    "            \n",
    "            n_rots = np.random.randint(low=1, high=5)\n",
    "            for i in range(n_rots):\n",
    "                img = np.rot90(img)         \n",
    "                if nl:\n",
    "                    labels_rot = torch.zeros_like(labels)\n",
    "                    labels_rot[..., 1] = labels[..., 2]          # Rotate x\n",
    "                    labels_rot[..., 2] = 1 - labels[..., 1]      # Rotate y\n",
    "                    labels_rot[..., 3] = labels[..., 4]          # Rotate width\n",
    "                    labels_rot[..., 4] = labels[..., 3]          # Rotate height\n",
    "                    \n",
    "                    labels = labels_rot\n",
    "                \n",
    "            # Flip up-down\n",
    "            if np.random.random() < FLIPUD:\n",
    "                img = np.flipud(img)\n",
    "                if nl:\n",
    "                    labels[:, 2] = 1 - labels[:, 2]  # y -> -y\n",
    "\n",
    "            # Flip left-right\n",
    "            if np.random.random() < FLIPLR:\n",
    "                img = np.fliplr(img)\n",
    "                if nl:\n",
    "                    labels[:, 1] = 1 - labels[:, 1]  # x -> -x\n",
    "        \n",
    "        img = torch.from_numpy(np.ascontiguousarray(img))\n",
    "        img = img.permute(2, 0, 1)\n",
    "        original_img = torch.from_numpy(np.ascontiguousarray(original_img))\n",
    "        original_img = original_img.permute(2, 0, 1)\n",
    "        \n",
    "        grid_size_x = img.size(dim=2) / S  # Images in PyTorch have size (channels, height, width)\n",
    "        grid_size_y = img.size(dim=1) / S\n",
    "\n",
    "        # Process bounding boxes into the SxSx(5*B+C) ground truth tensor\n",
    "        boxes = {}\n",
    "        class_names = {}                    # Track what class each grid cell has been assigned to\n",
    "        depth = 5 * B + C                   # 5 numbers per bbox, then one-hot encoding of label\n",
    "        ground_truth = torch.zeros((S, S, depth))\n",
    "        \n",
    "        for name, x, y, width, height in labels: \n",
    "            assert name.item() in self.classes, f\"Unrecognized class '{name.item()}'\"\n",
    "            class_index = self.classes[name.item()]\n",
    "\n",
    "            # Calculate the position of the center of the bounding box\n",
    "            mid_x = x * IMAGE_SIZE[0]\n",
    "            mid_y = y * IMAGE_SIZE[1]\n",
    "            col = int(mid_x // grid_size_x)\n",
    "            row = int(mid_y // grid_size_y)\n",
    "\n",
    "            if 0 <= col < S and 0 <= row < S:\n",
    "                cell = (row, col)\n",
    "                if cell not in class_names or name == class_names[cell]:\n",
    "\n",
    "                    # Insert class one-hot encoding into ground truth\n",
    "                    one_hot = torch.zeros(C)      \n",
    "                    one_hot[class_index] = 1.0\n",
    "                    ground_truth[row, col, :C] = one_hot\n",
    "                    class_names[cell] = name\n",
    "\n",
    "                    # Insert bounding box into ground truth tensor\n",
    "                    bbox_index = boxes.get(cell, 0)\n",
    "                    if bbox_index < B:\n",
    "                        bbox_truth = (\n",
    "                            (mid_x - col * grid_size_x) / IMAGE_SIZE[0],            # x coord relative to grid square\n",
    "                            (mid_y - row * grid_size_y) / IMAGE_SIZE[1],            # y coord relative to grid square\n",
    "                            width,                                                  # Width\n",
    "                            height,                                                 # Height\n",
    "                            1.0                                                     # Confidence\n",
    "                        )\n",
    "\n",
    "                        # Fill all bbox slots with the current bbox (starting from the current bbox slot, avoiding overriding prev)\n",
    "                        # This prevents having \"dead\" boxes (zeros) at the end, which messes up IOU loss calculations\n",
    "                        bbox_start = 5 * bbox_index + C\n",
    "                        ground_truth[row, col, bbox_start:] = torch.tensor(bbox_truth).repeat(B - bbox_index)\n",
    "                        boxes[cell] = bbox_index + 1\n",
    "\n",
    "        return img, ground_truth, original_img\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bec4a70",
   "metadata": {},
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43db840a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bbox_attr(data, i):\n",
    "    \"\"\"Returns the Ith attribute of each bounding box in data.\"\"\"\n",
    "\n",
    "    attr_start = C + i\n",
    "    return data[..., attr_start::5]\n",
    "\n",
    "def xywh2xyxy(t):\n",
    "    \"\"\"Changes format of bounding boxes from [x, y, width, height] to ([x1, y1], [x2, y2]).\"\"\"\n",
    "\n",
    "    width = bbox_attr(t, 2)\n",
    "    x = bbox_attr(t, 0)\n",
    "    x1 = x - width / 2.0\n",
    "    x2 = x + width / 2.0\n",
    "\n",
    "    height = bbox_attr(t, 3)\n",
    "    y = bbox_attr(t, 1)\n",
    "    y1 = y - height / 2.0\n",
    "    y2 = y + height / 2.0\n",
    "\n",
    "    return torch.stack((x1, y1), dim=4), torch.stack((x2, y2), dim=4)\n",
    "\n",
    "def get_iou(p, a):\n",
    "    p_tl, p_br = xywh2xyxy(p)          # (batch, S, S, B, 2)\n",
    "    a_tl, a_br = xywh2xyxy(a)\n",
    "\n",
    "    # Largest top-left corner and smallest bottom-right corner give the intersection\n",
    "    coords_join_size = (-1, -1, -1, B, B, 2)\n",
    "    tl = torch.max(\n",
    "        p_tl.unsqueeze(4).expand(coords_join_size),         # (batch, S, S, B, 1, 2) -> (batch, S, S, B, B, 2)\n",
    "        a_tl.unsqueeze(3).expand(coords_join_size)          # (batch, S, S, 1, B, 2) -> (batch, S, S, B, B, 2)\n",
    "    )\n",
    "    br = torch.min(\n",
    "        p_br.unsqueeze(4).expand(coords_join_size),\n",
    "        a_br.unsqueeze(3).expand(coords_join_size)\n",
    "    )\n",
    "\n",
    "    intersection_sides = torch.clamp(br - tl, min=0.0)\n",
    "    intersection = intersection_sides[..., 0] \\\n",
    "                   * intersection_sides[..., 1]       # (batch, S, S, B, B)\n",
    "\n",
    "    p_area = bbox_attr(p, 2) * bbox_attr(p, 3)                  # (batch, S, S, B)\n",
    "    p_area = p_area.unsqueeze(4).expand_as(intersection)        # (batch, S, S, B, 1) -> (batch, S, S, B, B)\n",
    "\n",
    "    a_area = bbox_attr(a, 2) * bbox_attr(a, 3)                  # (batch, S, S, B)\n",
    "    a_area = a_area.unsqueeze(3).expand_as(intersection)        # (batch, S, S, 1, B) -> (batch, S, S, B, B)\n",
    "\n",
    "    union = p_area + a_area - intersection\n",
    "\n",
    "    # Catch division-by-zero\n",
    "    zero_unions = (union == 0.0)\n",
    "    union[zero_unions] = EPSILON\n",
    "    intersection[zero_unions] = 0.0\n",
    "\n",
    "    return intersection / union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff1c6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YoloLoss(nn.Module): \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l_coord = 5\n",
    "        self.l_noobj = 0.5\n",
    "\n",
    "    def forward(self, p, a):\n",
    "        # Calculate IOU of each predicted bbox against the ground truth bbox\n",
    "        iou = get_iou(p, a)                     # (batch, S, S, B, B)\n",
    "        max_iou = torch.max(iou, dim=-1)[0]     # (batch, S, S, B)\n",
    "\n",
    "        # Get masks\n",
    "        bbox_mask = bbox_attr(a, 4) > 0.0\n",
    "        p_template = bbox_attr(p, 4) > 0.0\n",
    "        obj_i = bbox_mask[..., 0:1]         # 1 if grid I has any object at all\n",
    "        responsible = torch.zeros_like(p_template).scatter_(       # (batch, S, S, B)\n",
    "            -1,\n",
    "            torch.argmax(max_iou, dim=-1, keepdim=True),                # (batch, S, S, B)\n",
    "            value=1                         # 1 if bounding box is \"responsible\" for predicting the object\n",
    "        )\n",
    "        obj_ij = obj_i * responsible        # 1 if object exists AND bbox is responsible\n",
    "        noobj_ij = ~obj_ij                  # Otherwise, confidence should be 0\n",
    "\n",
    "        # XY position losses\n",
    "        x_losses = mse_loss(\n",
    "            obj_ij * bbox_attr(p, 0),\n",
    "            obj_ij * bbox_attr(a, 0)\n",
    "        )\n",
    "        y_losses = mse_loss(\n",
    "            obj_ij * bbox_attr(p, 1),\n",
    "            obj_ij * bbox_attr(a, 1)\n",
    "        )\n",
    "        pos_losses = x_losses + y_losses\n",
    "        # print('pos_losses', pos_losses.item())\n",
    "\n",
    "        # Bbox dimension losses\n",
    "        p_width = bbox_attr(p, 2)\n",
    "        a_width = bbox_attr(a, 2)\n",
    "        width_losses = mse_loss(\n",
    "            obj_ij * torch.sign(p_width) * torch.sqrt(torch.abs(p_width) + EPSILON),\n",
    "            obj_ij * torch.sqrt(a_width)\n",
    "        )\n",
    "        p_height = bbox_attr(p, 3)\n",
    "        a_height = bbox_attr(a, 3)\n",
    "        height_losses = mse_loss(\n",
    "            obj_ij * torch.sign(p_height) * torch.sqrt(torch.abs(p_height) + EPSILON),\n",
    "            obj_ij * torch.sqrt(a_height)\n",
    "        )\n",
    "        dim_losses = width_losses + height_losses\n",
    "        # print('dim_losses', dim_losses.item())\n",
    "\n",
    "        # Confidence losses (target confidence is IOU)\n",
    "        obj_confidence_losses = mse_loss(\n",
    "            obj_ij * bbox_attr(p, 4),\n",
    "            obj_ij * torch.ones_like(max_iou)\n",
    "        )\n",
    "        # print('obj_confidence_losses', obj_confidence_losses.item())\n",
    "        noobj_confidence_losses = mse_loss(\n",
    "            noobj_ij * bbox_attr(p, 4),\n",
    "            torch.zeros_like(max_iou)\n",
    "        )\n",
    "        # print('noobj_confidence_losses', noobj_confidence_losses.item())\n",
    "\n",
    "        # Classification losses\n",
    "        class_losses = mse_loss(\n",
    "            obj_i * p[..., :C],\n",
    "            obj_i * a[..., :C]\n",
    "        )\n",
    "        # print('class_losses', class_losses.item())\n",
    "\n",
    "        total = self.l_coord * (pos_losses + dim_losses) \\\n",
    "                + obj_confidence_losses \\\n",
    "                + self.l_noobj * noobj_confidence_losses \\\n",
    "                + class_losses\n",
    "        \n",
    "        return total / BATCH_SIZE\n",
    "\n",
    "\n",
    "def mse_loss(a, b):\n",
    "    flattened_a = torch.flatten(a, end_dim=-2)\n",
    "    flattened_b = torch.flatten(b, end_dim=-2).expand_as(flattened_a)\n",
    "    return F.mse_loss(\n",
    "        flattened_a,\n",
    "        flattened_b,\n",
    "        reduction='sum'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3bff3a",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bd19c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLOv1(nn.Module):\n",
    "    def __init__(self, n_channels=6):\n",
    "        super().__init__()\n",
    "        self.depth = B * 5 + C\n",
    "\n",
    "        layers = [\n",
    "            nn.Conv2d(n_channels, 64, kernel_size=7, stride=2, padding=3),          \n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(64, 192, kernel_size=3, padding=1),                           \n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(192, 128, kernel_size=1),                                     \n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "            nn.Conv2d(256, 256, kernel_size=1),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        ]\n",
    "\n",
    "        for _ in range(4):                                                          \n",
    "            layers += [\n",
    "                nn.Conv2d(512, 256, kernel_size=1),\n",
    "                nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "                nn.LeakyReLU(negative_slope=0.1)\n",
    "            ]\n",
    "            \n",
    "        layers += [\n",
    "            nn.Conv2d(512, 512, kernel_size=1),\n",
    "            nn.Conv2d(512, 1024, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        ]\n",
    "\n",
    "        for _ in range(2):                                                          \n",
    "            layers += [\n",
    "                nn.Conv2d(1024, 512, kernel_size=1),\n",
    "                nn.Conv2d(512, 1024, kernel_size=3, padding=1),\n",
    "                nn.LeakyReLU(negative_slope=0.1)\n",
    "            ]\n",
    "            \n",
    "        layers += [\n",
    "            nn.Conv2d(1024, 1024, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "            nn.Conv2d(1024, 1024, kernel_size=3, stride=2, padding=1),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "        ]\n",
    "\n",
    "        for _ in range(2):                                                          \n",
    "            layers += [\n",
    "                nn.Conv2d(1024, 1024, kernel_size=3, padding=1),\n",
    "                nn.LeakyReLU(negative_slope=0.1)\n",
    "            ]\n",
    "\n",
    "        layers += [\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(S * S * 1024, 1024),                                           \n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "            nn.Linear(1024, S * S * self.depth),                                    \n",
    "        ]\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.Tensor(x).to(torch.float)    # x.shape is [batch_size, n_channels, height, width]\n",
    "        res = self.model.forward(x)\n",
    "        return torch.reshape(res, (x.size(dim=0), S, S, self.depth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd72c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLOv1(n_channels=6)\n",
    "x = torch.randn((32, 6, 448, 448))\n",
    "res = model(x)\n",
    "res.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d7d5a9",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6868883f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2727c515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %time\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.autograd.set_detect_anomaly(True)         # Check for nan loss\n",
    "writer = SummaryWriter()\n",
    "now = datetime.now()\n",
    "\n",
    "model = YOLOv1Lite8(n_channels=6).to(device)\n",
    "loss_function = YoloLoss()\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Learning rate scheduler \n",
    "# scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=utils.scheduler_lambda)\n",
    "\n",
    "folder = 'mocks_6d_crop_norm_64_0to255_VELA6_toy_clumps' #_v2 # 'vela3_6d_crop_64_0to255' # /128\n",
    "\n",
    "# Load the dataset \n",
    "images_train = f'/sci/labs/dekel/oshri.fatkiev/{folder}/images/train'\n",
    "labels_train = f'/sci/labs/dekel/oshri.fatkiev/{folder}/labels/train'\n",
    "train_set = YoloDataset(images_path=images_train, labels_path=labels_train, augment=True)\n",
    "\n",
    "images_test = f'/sci/labs/dekel/oshri.fatkiev/{folder}/images/val'\n",
    "labels_test = f'/sci/labs/dekel/oshri.fatkiev/{folder}/labels/val'\n",
    "val_set = YoloDataset(images_path=images_test, labels_path=labels_test, augment=True)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_set,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=4,\n",
    "    persistent_workers=True,\n",
    "    drop_last=True,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_set,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=4,\n",
    "    persistent_workers=True,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "# Create folders\n",
    "root = os.path.join('models', 'yolo_v1', now.strftime('%m_%d_%Y'), now.strftime('%H_%M_%S'))\n",
    "weight_dir = os.path.join(root, 'weights')\n",
    "if not os.path.isdir(weight_dir): os.makedirs(weight_dir)\n",
    "\n",
    "# Metrics\n",
    "train_losses, train_errors = np.empty((2, 0)), np.empty((2, 0))\n",
    "val_losses, val_errors = np.empty((2, 0)), np.empty((2, 0))\n",
    "\n",
    "def save_metrics():\n",
    "    np.save(os.path.join(root, 'train_losses'), train_losses)\n",
    "    np.save(os.path.join(root, 'val_losses'), val_losses)\n",
    "    np.save(os.path.join(root, 'train_errors'), train_errors)\n",
    "    np.save(os.path.join(root, 'val_errors'), val_errors)\n",
    "\n",
    "    \n",
    "l_trn, l_val = [], []\n",
    "for epoch in tqdm(range(WARMUP_EPOCHS + EPOCHS), desc='Epoch'):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for data, labels, _ in tqdm(train_loader, desc='Train', position=0, leave=True, colour='green'):\n",
    "        # print(f'data:{data.shape}, labels: {labels.shape}')\n",
    "        data = data.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model.forward(data)\n",
    "        # print(f'prediction shape {predictions.size()} true shape {labels.size()}')\n",
    "        loss = loss_function(predictions, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() / len(train_loader)\n",
    "        del data, labels, predictions\n",
    "\n",
    "    # Step and graph scheduler once an epoch\n",
    "    # writer.add_scalar('Learning Rate', scheduler.get_last_lr()[0], epoch)\n",
    "    # scheduler.step()\n",
    "\n",
    "    train_losses = np.append(train_losses, [[epoch], [train_loss]], axis=1)\n",
    "    writer.add_scalar('Loss/train', train_loss, epoch)\n",
    "\n",
    "    l_trn.append(train_loss)\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0\n",
    "            for data, labels, _ in tqdm(val_loader, desc='Val', position=0, leave=True):\n",
    "                data = data.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                predictions = model.forward(data)\n",
    "                loss = loss_function(predictions, labels)\n",
    "\n",
    "                val_loss += loss.item() / len(val_loader)\n",
    "                del data, labels, predictions\n",
    "        val_losses = np.append(val_losses, [[epoch], [val_loss]], axis=1)\n",
    "        writer.add_scalar('Loss/val', val_loss, epoch)\n",
    "        save_metrics()\n",
    "\n",
    "        print(f'\\nepoch: {epoch}/{EPOCHS} Loss/train: {train_loss:.3f}, Loss/val: {val_loss:.3f}')\n",
    "        l_val.append(val_loss)\n",
    "\n",
    "save_metrics()\n",
    "torch.save(model.state_dict(), os.path.join(weight_dir, 'final'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47dbfc7f",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd85b104",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_precision_recall_with_nms(true_boxes, pred_boxes, scores, iou_thres=0.5, nms_thres=0.3):\n",
    "    \n",
    "    true_boxes = true_boxes.float()\n",
    "    pred_boxes = pred_boxes.float()\n",
    "\n",
    "    # Apply NMS to predicted boxes\n",
    "    keep_indices = nms(pred_boxes, scores=scores.float(), iou_threshold=nms_thres)\n",
    "    pred_boxes = pred_boxes[keep_indices]\n",
    "\n",
    "    # Calculate IoU  with non-maximum suppressed predictions\n",
    "    # I'th row with j'th column represents iou between pred[i,:] and true[j,:]\n",
    "    iou_matrix = distance_box_iou(pred_boxes, true_boxes)\n",
    "    # iou_matrix = box_iou(pred_boxes, true_boxes)\n",
    "    \n",
    "    # Identify true positives, false positives, and false negatives\n",
    "    tp = (iou_matrix >= iou_thres).sum(dim=1)\n",
    "    fp = (tp == 0).sum()\n",
    "    fn = (iou_matrix.max(dim=0).values < iou_thres).sum()\n",
    "    \n",
    "    # Convert tp from list of zeros and ones to the sum of all the ones in the list\n",
    "    tp = tp.sum()\n",
    "    \n",
    "    # Calculate precision and recall\n",
    "    precision = tp.float() / (tp + fp)\n",
    "    recall = tp.float() / (tp + fn)\n",
    "\n",
    "    return precision.item(), recall.item()\n",
    "\n",
    "\n",
    "def get_bboxes_xyxy(pred, grid_size_x, grid_size_y, conf_thres=0.2):\n",
    "    \n",
    "    m = pred.size(dim=0) \n",
    "    n = pred.size(dim=1) \n",
    "    \n",
    "    bboxes = []\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            for k in range((pred.size(dim=2) - C) // 5):\n",
    "                bbox_start = 5 * k + C\n",
    "                bbox_end = 5 * (k + 1) + C\n",
    "                bbox = pred[i, j, bbox_start:bbox_end]\n",
    "                class_index = torch.argmax(pred[i, j, :C]).item()\n",
    "                confidence = pred[i, j, class_index].item() * bbox[4].item()          # pr(c) * IOU\n",
    "                if confidence < conf_thres:\n",
    "                    continue\n",
    "                x = (bbox[0] * IMAGE_SIZE[0]) + (j * grid_size_x)\n",
    "                y = (bbox[1] * IMAGE_SIZE[1]) + (i * grid_size_y)\n",
    "                width = bbox[2] * IMAGE_SIZE[0]\n",
    "                height = bbox[3] * IMAGE_SIZE[1]\n",
    "                b = [x-width/2, y-height/2, x+width/2, y+height/2, confidence, class_index]\n",
    "                if b not in bboxes:\n",
    "                    bboxes.append(b)       \n",
    "\n",
    "    return torch.tensor(bboxes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5220edc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "torch.cuda.manual_seed(1)\n",
    "\n",
    "classes = [0]\n",
    "\n",
    "images_test = '/sci/labs/dekel/oshri.fatkiev/vela3_6d_crop_64_0to255/images/test'\n",
    "labels_test = '/sci/labs/dekel/oshri.fatkiev/vela3_6d_crop_64_0to255/labels/test'\n",
    "\n",
    "dataset = YoloDataset(images_path=images_test, labels_path=labels_test)\n",
    "dataloader = DataLoader(dataset, batch_size=16)\n",
    "\n",
    "model = YOLOv1Lite8(n_channels=6)\n",
    "model.eval()\n",
    "model.load_state_dict(torch.load(os.path.join(weight_dir, 'final')))#, map_location=torch.device('cpu')))\n",
    "                      \n",
    "precision, recall, count = 0, 0, 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for image, labels, original in tqdm(dataloader, colour='green'):\n",
    "        \n",
    "        res = model.forward(image)\n",
    "        batch_size = image.size(dim=0)\n",
    "        \n",
    "        for idx in range(batch_size):\n",
    "\n",
    "            grid_size_x = image.size(dim=3) / S\n",
    "            grid_size_y = image.size(dim=2) / S\n",
    "            \n",
    "            pred = res[idx,...]\n",
    "            pred_bboxes = get_bboxes_xyxy(pred, grid_size_x, grid_size_y, conf_thres=0.2)\n",
    "    \n",
    "            labels_copy = labels[idx,...]\n",
    "            true_bboxes = get_bboxes_xyxy(labels_copy, grid_size_x, grid_size_y, conf_thres=1)\n",
    "            \n",
    "            if len(pred_bboxes) > 0:\n",
    "                p_temp, r_temp = calculate_precision_recall_with_nms(\n",
    "                    true_bboxes[...,:4],\n",
    "                    pred_bboxes[...,:4],\n",
    "                    pred_bboxes[...,5], \n",
    "                    iou_thres=0.3, \n",
    "                    nms_thres=0.5\n",
    "                )\n",
    "            elif len(pred_bboxes) == 0 and len(true_bboxes) == 0:\n",
    "                p_temp, r_temp = 1, 1\n",
    "            elif len(pred_bboxes) == 0 and len(true_bboxes) > 0: \n",
    "                p_temp, r_temp = 0, 0\n",
    "                \n",
    "            precision += p_temp\n",
    "            recall += r_temp\n",
    "            count += 1\n",
    "        \n",
    "print(f'Avg. Precision: {precision/count:.3f}, Avg. Recall: {recall/count:.3f}, based on {count} images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843fbf66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
